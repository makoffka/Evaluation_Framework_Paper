# Evaluation_Framework_Paper

I have used 3 datasets: Cirrhosis, Diabetes and Stroke. They are in the Data folder
I had 4 models: TVAE, CTGAN, GReaT, and RTF
2 of them TVAE and CTGAN - are very quick and easy to run from VScode. 
RTF and GReaT require more resources and I was running them on Google Colab with T4.
I repeated the same evaluation process for each of them.
In addition I did TSTR where I took generated Synthetic data and used as a Training data to see if the Fake data is useable for the downstream tasks.
Then I did the same evaluation only for TSTR dataset - trained on synthetic tested on real testing data.

I also checked each synthetic dataset for out of range values using great_expectations package. 
I think I did it separately - due to that it's not included in the code provided except for RTF_cirrhosis (It throws an error there. Likely due to some change in the package. It worked in May)

# **Synthetic Data Evaluation Framework in Healthcare**

### **Overview**

This repository provides a **comprehensive framework** for evaluating the quality of synthetic healthcare data generated by various machine learning models. The project addresses the growing need for synthetic datasets to protect patient privacy while ensuring the usability of the data in downstream machine learning applications. By synthesizing healthcare data, such as the diabetes dataset, the framework focuses on maintaining data reality, diversity, quality, privacy, and fairnessâ€”crucial factors when dealing with sensitive healthcare information.

The framework evaluates synthetic data across multiple dimensions, including statistical, analytical, and fairness metrics. It also tracks computational costs such as memory usage and processing time, offering insights for healthcare practitioners and researchers interested in generating and using synthetic data in real-world scenarios.

### **Key Features**
- **Synthetic Data Generation**: Leverages state-of-the-art machine learning models, including Bayesian Networks, GANs, and flow-based models (e.g., RealNVP).
- **Comprehensive Evaluation**: Benchmarks synthetic data on five dimensions: reality, diversity, quality, privacy, and usability in machine learning tasks.
- **Fairness Assessment**: Incorporates fairness metrics such as Demographic Parity, Disparate Impact, and Equal Opportunity to ensure the generated data does not reflect bias.
- **Computational Cost Monitoring**: Tracks memory usage and processing time during synthetic data generation.
- **Visualization Tools**: Generates visualizations (e.g., histograms, mean comparison) to compare real and synthetic datasets.

### **Project Goals**
1. **Provide a Benchmark Framework**: Evaluate synthetic healthcare data generated by diverse models.
2. **Preserve Privacy**: Ensure synthetic data maintains privacy while accurately representing real patient data.
3. **Enable Usability**: Generate data that is usable for machine learning tasks and downstream analytics.
4. **Assess Fairness**: Ensure that synthetic data does not propagate or amplify bias in healthcare settings.
5. **Measure Computational Efficiency**: Offer insights into the computational cost of generating synthetic data.

---

## **Table of Contents**
1. [Installation](#installation)
2. [Usage](#usage)
3. [Evaluation Framework](#evaluation-framework)
   - Reality, Diversity, Quality, and Privacy
   - Fairness Metrics
4. [Visualization Tools](#visualization-tools)
5. [Computational Complexity](#computational-complexity)
6. [Contributing](#contributing)
7. [License](#license)

---

## **Installation**

Clone the repository and install the required dependencies using `pip`:

```bash
git clone https://github.com/your-username/synthetic-healthcare-evaluation.git
cd synthetic-healthcare-evaluation
pip install -r requirements.txt

---


